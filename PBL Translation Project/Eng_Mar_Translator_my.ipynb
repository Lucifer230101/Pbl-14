{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjzU4ZYFn-VM"
   },
   "source": [
    "Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "elJeh1qUntk6"
   },
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "# from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZYiGc3E9T18f"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount(‘/content/gdrive’)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTFa1pXss61d"
   },
   "source": [
    "importing dataset and parsing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nJ-hiOD_qa4N",
    "outputId": "3edc6c91-fad7-4413-cce1-65a4f198cc5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['एनजीसी  १४२७ए ( NGC 1427A ) , ५.२ कोटि प्रकाशवर्षे अंतरावरील  आकारहीन  दीर्घिका .\\tNGC 1427A , an example of an irregular galaxy . It is an Irr-I category galaxy about 52 Mly distant .', 'सर्पिलाकार किंवा लंबवर्तुळाकार दीर्घिकांप्रमाणे  निश्चित आकार नसणाऱ्या दीर्घिकांना आकारहीन दीर्घिका ( इंग्रजी : Irregular galaxy - इर्रेग्यूलर गॅलॅक्सी ) म्हणतात . [ 1 ] आकारहीन दीर्घिकांचा आकार असामान्य असतो . ते हबल अनुक्रमाच्या कोणत्याही नियमित गटात बसत नाहीत आणि त्यांच्यामध्ये तेजोगोल ही नसतो व सर्पिलाकार फाटेही नसतात . [ 2 ]\\tAn irregular galaxy is a galaxy that does not have a distinct regular shape , unlike a spiral or an elliptical galaxy.[1 ] The shape of an irregular galaxy is uncommon – they do not fall into any of the regular classes of the Hubble sequence , and they are often chaotic in appearance , with neither a nuclear bulge nor any trace of spiral arm structure.[2 ]', 'एकूण दीर्घिकांपैकी यांची संख्या एकत्रितपणे एक चतुर्थांश आहे असे मानले जाते . काही आकारहीन दीर्घिका एकेकाळी सर्पिलाकार किंवा लंबवर्तुळाकार होत्या , पण गुरुत्वीय बलातील विषमतेमुळे त्यांचा आकार अनियमित झाला . आकारहीन दीर्घिकांमध्ये  विपुल  प्रमाणात वायु व  धुळ असू  शकते . [ 3 ]  हे बटू ( ठेंगण्या ) आकारहीन दीर्घिकांसाठी अपरिहार्यपणे  खरे नाही . [ 4 ]\\tCollectively they are thought to make up about a quarter of all galaxies . Some irregular galaxies were once spiral or elliptical galaxies but were deformed by disorders in gravitational pull . Irregular galaxies may contain abundant[3 ] amounts of gas and dust . This is not necessarily true for Dwarf Irregulars.[4 ]', 'Types\\tTypes', 'आकारहीन दीर्घिकांचे तीन मुख्य प्रकार : [ 5 ]\\tThere are three major types of irregular galaxies:[5 ]', 'आयआरआर-१ ( Irr I ) दीर्घिका : या प्रकारच्या दीर्घिकेचा आकार थोड्या प्रमाणात रचनाबद्ध असतो , पण हबल अनुक्रमाmadhye वर्गीकरण करण्याएवढा स्पष्ट नसतो .  काही  प्रमाणात  सर्पिलाकार  रचना  असणाऱ्या  उपप्रकाराला  एसएम ( Sm ) दीर्घिका  म्हणतात .  सर्पिलाकार  रचना  नसणाऱ्या  उपप्रकाराला  आयएम ( Im ) दीर्घिका  म्हणतात .\\tAn Irr-I galaxy ( Irr I ) is an irregular galaxy that features some structure but not enough to place it cleanly into the Hubble sequence . Subtypes with some spiral structure are called Sm galaxies Subtypes without spiral structure are called Im galaxies .', 'आयआरआर-२ ( Irr II ) दीर्घिका : या प्रकारच्या दीर्घिकांमध्ये कोणत्याही प्रकारची रचना नसते ज्याच्या आधारे त्यांचे हबल अनुक्रमामध्ये वर्गीकरण करता .\\tAn Irr-II galaxy ( Irr II ) is an irregular galaxy that does not appear to feature any structure that can place it into the Hubble sequence .', 'डीएल-दीर्घिका ( dIrrs ) : ही बटू आकारहीन दीर्घिका आहे . [ 6  या प्रकारच्या दीर्घिका एकूणच दीर्घिकांच्या उत्क्रांतीचे  आकलन होण्यासाठी महत्वाच्या मानल्या जातात , कारण शक्यतो  त्यांची मेटॅलिसिटी कमी असते , त्यांच्यामध्ये वायू व धुळीचे प्रमाण तुलनेने जास्त असते आणि या दीर्घिका  विश्वातील सर्वात पहिल्या दीर्घिकांसारख्या आहेत असे मानले जाते .\\tA dI-galaxy ( or dIrrs ) is a dwarf irregular galaxy[6 ] This type of galaxy is now thought to be important to understand the overall evolution of galaxies , as they tend to have a low level of metallicity and relatively high levels of gas , and are thought to be similar to the earliest galaxies that populated the Universe . They may represent a local ( and therefore more recent ) version of the faint blue galaxies known to exist in deep field galaxy surveys .', 'मॅजेलॅनिक\\tMagellanic Clouds', 'मॅजेलॅनिक ढग दीर्घिकांचे पूर्वी आकारहीन दीर्घिकांमध्ये वर्गीकरण केले जायचे . नंतर  मोठ्या  मॅजेलॅनिक  ढगाचे  वर्गीकरण  एसबीएम ( SBm ) [ 7 ]  या भुजायुक्त सर्पिलाकार दीर्घिकेच्या एका प्रकारामध्ये करण्यात आले .  लहान  मॅजेलॅनिक ढगाचे वर्गीकरण  आकारहीन  दीर्घिका  प्रकार  आयएम ( Im )  असेच  केले  जाते .\\tThe Magellanic Cloud galaxies were once classified as irregular galaxies . The Large Magellanic Cloud has since been re-classified as type SBm [ 7 ] a type of barred spiral galaxy , the barred Magellanic spiral type . The Small Magellanic Cloud remains classified as an irregular galaxy of type Im under current Galaxy morphological classification , although it does contain a bar structure . Therefore , newer classification schemes place the SMC outside the irregular class as well .']\n"
     ]
    }
   ],
   "source": [
    "# for Dataset1\n",
    "with open('Dataset1.txt', encoding=\"utf8\") as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "    print(lines[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8AuWcIXMrGK-"
   },
   "outputs": [],
   "source": [
    "text_pairs = []\n",
    "for line in lines:\n",
    "    #print(line)\n",
    "    mar, eng = line.split(\"\\t\")[:2]     #spliting with tab and taking first 2 cols\n",
    "    #print(eng)\n",
    "    #print(mar)\n",
    "    mar = \"[start] \" + mar + \" [end]\"\n",
    "    text_pairs.append((eng, mar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1gwyS8qCsGaf",
    "outputId": "28c9302f-a9ed-42df-af39-1c9f9921cf5e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('An Irr-I galaxy ( Irr I ) is an irregular galaxy that features some structure but not enough to place it cleanly into the Hubble sequence . Subtypes with some spiral structure are called Sm galaxies Subtypes without spiral structure are called Im galaxies .',\n",
       "  '[start] आयआरआर-१ ( Irr I ) दीर्घिका : या प्रकारच्या दीर्घिकेचा आकार थोड्या प्रमाणात रचनाबद्ध असतो , पण हबल अनुक्रमाmadhye वर्गीकरण करण्याएवढा स्पष्ट नसतो .  काही  प्रमाणात  सर्पिलाकार  रचना  असणाऱ्या  उपप्रकाराला  एसएम ( Sm ) दीर्घिका  म्हणतात .  सर्पिलाकार  रचना  नसणाऱ्या  उपप्रकाराला  आयएम ( Im ) दीर्घिका  म्हणतात . [end]'),\n",
       " ('An Irr-II galaxy ( Irr II ) is an irregular galaxy that does not appear to feature any structure that can place it into the Hubble sequence .',\n",
       "  '[start] आयआरआर-२ ( Irr II ) दीर्घिका : या प्रकारच्या दीर्घिकांमध्ये कोणत्याही प्रकारची रचना नसते ज्याच्या आधारे त्यांचे हबल अनुक्रमामध्ये वर्गीकरण करता . [end]'),\n",
       " ('A dI-galaxy ( or dIrrs ) is a dwarf irregular galaxy[6 ] This type of galaxy is now thought to be important to understand the overall evolution of galaxies , as they tend to have a low level of metallicity and relatively high levels of gas , and are thought to be similar to the earliest galaxies that populated the Universe . They may represent a local ( and therefore more recent ) version of the faint blue galaxies known to exist in deep field galaxy surveys .',\n",
       "  '[start] डीएल-दीर्घिका ( dIrrs ) : ही बटू आकारहीन दीर्घिका आहे . [ 6  या प्रकारच्या दीर्घिका एकूणच दीर्घिकांच्या उत्क्रांतीचे  आकलन होण्यासाठी महत्वाच्या मानल्या जातात , कारण शक्यतो  त्यांची मेटॅलिसिटी कमी असते , त्यांच्यामध्ये वायू व धुळीचे प्रमाण तुलनेने जास्त असते आणि या दीर्घिका  विश्वातील सर्वात पहिल्या दीर्घिकांसारख्या आहेत असे मानले जाते . [end]'),\n",
       " ('Magellanic Clouds', '[start] मॅजेलॅनिक [end]'),\n",
       " ('The Magellanic Cloud galaxies were once classified as irregular galaxies . The Large Magellanic Cloud has since been re-classified as type SBm [ 7 ] a type of barred spiral galaxy , the barred Magellanic spiral type . The Small Magellanic Cloud remains classified as an irregular galaxy of type Im under current Galaxy morphological classification , although it does contain a bar structure . Therefore , newer classification schemes place the SMC outside the irregular class as well .',\n",
       "  '[start] मॅजेलॅनिक ढग दीर्घिकांचे पूर्वी आकारहीन दीर्घिकांमध्ये वर्गीकरण केले जायचे . नंतर  मोठ्या  मॅजेलॅनिक  ढगाचे  वर्गीकरण  एसबीएम ( SBm ) [ 7 ]  या भुजायुक्त सर्पिलाकार दीर्घिकेच्या एका प्रकारामध्ये करण्यात आले .  लहान  मॅजेलॅनिक ढगाचे वर्गीकरण  आकारहीन  दीर्घिका  प्रकार  आयएम ( Im )  असेच  केले  जाते . [end]')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pairs[5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Go.\\tजा.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #3138228 (sabretou)', 'Run!\\tपळ!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #3138217 (sabretou)', 'Run!\\tधाव!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #3138218 (sabretou)', 'Run!\\tपळा!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #3138219 (sabretou)', 'Run!\\tधावा!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #3138220 (sabretou)', 'Who?\\tकोण?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2083030 (CK) & #3138225 (sabretou)', 'Wow!\\tवाह!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #52027 (Zifre) & #6728118 (sabretou)', 'Duck!\\tखाली वाका!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #280158 (CM) & #7731217 (Nativemarathi)', 'Fire!\\tआग!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1829639 (Spamster) & #3232248 (sabretou)', 'Fire!\\tफायर!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1829639 (Spamster) & #3232249 (sabretou)']\n"
     ]
    }
   ],
   "source": [
    "# for Dataset2\n",
    "with open('Dataset2.txt', encoding=\"utf8\") as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "    print(lines[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in lines:\n",
    "    #print(line)\n",
    "    eng, mar = line.split(\"\\t\")[:2]     #spliting with tab and taking first 2 cols\n",
    "    #print(eng)\n",
    "    #print(mar)\n",
    "    mar = \"[start] \" + mar + \" [end]\"\n",
    "    text_pairs.append((eng, mar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"I think it's unlikely that the next version of Windows will come out before the end of this month.\",\n",
       "  '[start] विंडोजची पुढची आवृत्ती या महीन्याच्या शेवटागोदर येईल हे मला असंभवनीय वाटतं. [end]'),\n",
       " (\"I think it's unlikely that the next version of Windows will come out before the end of this month.\",\n",
       "  '[start] विंडोजचा पुढचा वर्जन या महिन्याच्या शेवटागोदर येइल हे मला असंभाव्य वाटतं. [end]'),\n",
       " (\"I think it's unlikely that aliens similar to what we see in the movies have ever visited our planet.\",\n",
       "  '[start] चित्रपटांमध्ये दिसणार्\\u200dया परग्रहवासींसारखे परग्रहवासी आपल्या ग्रहावर कधी आले असतील हे मला असंभवनीय वाटतं. [end]'),\n",
       " (\"When the tempura I make cools down, it immediately loses its crispiness and doesn't taste very good.\",\n",
       "  '[start] जेव्हा मी बनवलेला तेम्पुरा थंड होतो, तेव्हा त्याचा कुरकुरीतपणा लगेच जातो आणि मग त्याची चव तेवढी चांगली लागत नाही. [end]'),\n",
       " ('If religion were synonymous with morality, Brazil would be the most uncorrupted country in the world.',\n",
       "  '[start] जर धर्म व नीतिमत्ता समानार्थी शब्द असते, तर ब्राजील जगातला सर्वात अभ्रष्ट देश असता. [end]'),\n",
       " (\"Just saying you don't like fish because of the bones is not really a good reason for not liking fish.\",\n",
       "  '[start] हड्डींमुळे मासे आवडत नाही असं म्हणणं हे काय मासे नावडण्यासाठी चांगलं कारण नाहीये. [end]'),\n",
       " (\"The Japanese Parliament today officially elected Ryūtarō Hashimoto as the country's 52nd prime minister.\",\n",
       "  '[start] आज जपानी संसदेने अधिकृतरित्या र्\\u200dयौतारौ हाशिमोतो यांना देशाचे ५२ावे पंतप्रधान म्हणून निवडले. [end]'),\n",
       " ('Tom tried to sell his old VCR instead of throwing it away, but no one would buy it, so he ended up throwing it away.',\n",
       "  '[start] टॉमने त्याचा जुना व्ही.सी.आर फेकून टाकण्याऐवजी विकून टाकण्याचा प्रयत्न केला, पण विकत घ्यायला कोणीच तयार नव्हतं, म्हणून त्याला तो शेवटी फेकूनच टाकायला लागला. [end]'),\n",
       " (\"You can't view Flash content on an iPad. However, you can easily email yourself the URLs of these web pages and view that content on your regular computer when you get home.\",\n",
       "  '[start] आयपॅडवर फ्लॅश आशय बघता येत नाही. पण तुम्ही त्या वेब पानांचे यूआरएल स्वतःला ईमेल करून तोच आशय घरी पोहोचल्यावर आपल्या रोजच्या संगणकावर पाहू शकता. [end]'),\n",
       " ('In 1969, Roger Miller recorded a song called \"You Don\\'t Want My Love.\" Today, this song is better known as \"In the Summer Time.\" It\\'s the first song he wrote and sang that became popular.',\n",
       "  '[start] १९६९मध्ये रॉजर मिलरने \"यू डोन्ट वॉन्ट माय लव्ह\" नावाचं गाणं रेकॉर्ड केलं. आज हे गाणं \"इन द समर टाइम\" म्हणून जास्त प्रसिद्ध आहे. त्याने लिहिलेलं व गायलेलं हे असं पहिलं गाणं होतं जे लोकप्रिय झालं. [end]')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pairs[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QiMoNtytmjp"
   },
   "source": [
    "split the sentence pairs into a training set, a validation set, and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MPUduUcVsKVh",
    "outputId": "e685da23-e5f5-4d02-902f-993f01ba83e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{97034} total pairs\n",
      "{67924} training pairs\n",
      "{14555} validation pairs\n",
      "{14555} test pairs\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))     # 15 % validation data \n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples # 70% training data\n",
    "train_pairs = text_pairs[:num_train_samples]    # first 70% data => training data\n",
    "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples] # 15% validation data\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples :] # 15% test data\n",
    "\n",
    "print({len(text_pairs)} ,\"total pairs\")\n",
    "print({len(train_pairs)} ,\"training pairs\")\n",
    "print({len(val_pairs)} ,\"validation pairs\")\n",
    "print({len(test_pairs)} ,\"test pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNrTXXHxw1ES"
   },
   "source": [
    "Vectorizing the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "7eR7jTQTw0DY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x00000258F390D4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x00000258F390D4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x00000258F3DE7678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x00000258F3DE7678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "strip_chars = string.punctuation \n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "vocab_size = 15000\n",
    "sequence_length = 20\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)    # lowercasing the sentence\n",
    "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")   # escape the punctuations\n",
    "\n",
    "\n",
    "eng_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\", \n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "mar_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "train_eng_texts = [pair[0] for pair in train_pairs]\n",
    "train_mar_texts = [pair[1] for pair in train_pairs]\n",
    "eng_vectorization.adapt(train_eng_texts)\n",
    "mar_vectorization.adapt(train_mar_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpqY9kOr04UU"
   },
   "source": [
    " format our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "04JumBcp0xod"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function format_dataset at 0x00000258F3E9A558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function format_dataset at 0x00000258F3E9A558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "def format_dataset(eng, mar):\n",
    "    eng = eng_vectorization(eng)    # vectorizing english sentences\n",
    "    mar = mar_vectorization(mar)    # vectorizing marathi sentences\n",
    "    return ({\"encoder_inputs\": eng, \"decoder_inputs\": mar[:, :-1],}, mar[:, 1:])\n",
    "\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    #print(pairs[:5]) # for reference\n",
    "    eng_texts, mar_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    mar_texts = list(mar_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, mar_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7K3hrgD32Ux"
   },
   "source": [
    "quick look at the sequence shapes.\n",
    "(we have batches of 64 pairs, and all sequences are 20 steps long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h_32mwqt3xSu",
    "outputId": "e2213ad6-79ca-4064-e37b-18cf0651f415"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[\"encoder_inputs\"].shape: (64, 20)\n",
      "inputs[\"decoder_inputs\"].shape: (64, 20)\n",
      "targets.shape: (64, 20)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
    "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6qHAjOi4bAu"
   },
   "source": [
    "Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "-ESBKEIA4BFN"
   },
   "outputs": [],
   "source": [
    "# sequence-to-sequence Transformer consisting of a TransformerEncoder and a TransformerDecoder chained together\n",
    "# reffered from keras library\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
    "        attention_output = self.attention(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
    "        )\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "\n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
    "        super(TransformerDecoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(out_2 + proj_output)\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(mask, mult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMi2btRz5LBq"
   },
   "source": [
    "assemble the end-to-end model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "8WX5cubM5M4N"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method PositionalEmbedding.call of <__main__.PositionalEmbedding object at 0x00000258F631CAC8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method PositionalEmbedding.call of <__main__.PositionalEmbedding object at 0x00000258F631CAC8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TransformerEncoder.call of <__main__.TransformerEncoder object at 0x00000258F62E1248>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TransformerEncoder.call of <__main__.TransformerEncoder object at 0x00000258F62E1248>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TransformerDecoder.call of <__main__.TransformerDecoder object at 0x00000258F8354E48>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TransformerDecoder.call of <__main__.TransformerDecoder object at 0x00000258F8354E48>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 256\n",
    "latent_dim = 1048 # ----------------2048 pahije\n",
    "num_heads = 8\n",
    "\n",
    "# Encoder structure : encoder_inputs => x (positional embedding) => encoder_outputs \n",
    "\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs) # positional aspect to input\n",
    "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x) \n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "# Decoder structure : decoder_inputs => \n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
    "\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "transformer = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjaDDL8E7aue"
   },
   "source": [
    "#Training our model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gPo7o7Uv7bX8",
    "outputId": "34580b77-723f-45e6-fc94-9ed7114dc1d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " positional_embedding (Position  (None, None, 256)   3845120     ['encoder_inputs[0][0]']         \n",
      " alEmbedding)                                                                                     \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " transformer_encoder (Transform  (None, None, 256)   2642456     ['positional_embedding[0][0]']   \n",
      " erEncoder)                                                                                       \n",
      "                                                                                                  \n",
      " model_1 (Functional)           (None, None, 15000)  12446640    ['decoder_inputs[0][0]',         \n",
      "                                                                  'transformer_encoder[0][0]']    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 18,934,216\n",
      "Trainable params: 18,934,216\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000258F85A0708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000258F85A0708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1062/1062 [==============================] - ETA: 0s - loss: 1.5189 - accuracy: 0.4159WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000258F6355438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000258F6355438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1062/1062 [==============================] - 1545s 1s/step - loss: 1.5189 - accuracy: 0.4159 - val_loss: 1.2576 - val_accuracy: 0.4895\n",
      "Epoch 2/2\n",
      "1062/1062 [==============================] - 1511s 1s/step - loss: 1.2766 - accuracy: 0.5015 - val_loss: 1.1184 - val_accuracy: 0.5339\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x258f6325888>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 2  # This should be at least 30 for convergence\n",
    "\n",
    "transformer.summary()\n",
    "transformer.compile(\n",
    "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LdDE-9bCSIE"
   },
   "source": [
    "saving the model in pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jrulmd50-eF6",
    "outputId": "d98650f4-50a0-47c3-ab77-0b56809bacc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x0000025881B1F558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x0000025881B1F558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000002588182F708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000002588182F708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, multi_head_attention_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://6e92cdb7-7680-4278-ab9b-a525fd846deb/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://6e92cdb7-7680-4278-ab9b-a525fd846deb/assets\n",
      "C:\\Users\\om\\anaconda3\\lib\\site-packages\\keras\\engine\\functional.py:1384: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "C:\\Users\\om\\anaconda3\\lib\\site-packages\\keras\\saving\\saved_model\\layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "pickle.dump(transformer, open('Eng_Mar_Translator.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yh5ATRmICO2n"
   },
   "source": [
    "Loading the model from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "8BCTwLq4BhDT"
   },
   "outputs": [],
   "source": [
    "# transformer = pickle.load(open('Eng_Mar_Translator.pkl','rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_GtzyAw8bOw"
   },
   "source": [
    "#Decoding test sentences\n",
    "{ feed into the model the vectorized English sentence as well as the target token \"[start]\", then we repeatedly generated the next token, until we hit the token \"[end]\".}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "2kluZsr38Bg0"
   },
   "outputs": [],
   "source": [
    "mar_vocab = mar_vectorization.get_vocabulary()\n",
    "mar_index_lookup = dict(zip(range(len(mar_vocab)), mar_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = mar_vectorization([decoded_sentence])[:, :-1]\n",
    "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
    "\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = mar_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "for _ in range(30):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    translated = decode_sequence(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7WdvegdXAW59",
    "outputId": "faeb8547-9193-4cbb-acb2-b85b6f9cf2db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following the adoption of the Government of India Act of 1858 , the Governor-General as representing the Crown became known as the Viceroy . The designation ' Viceroy ' , although it was most frequently used in ordinary parlance , had no statutory authority , and was never employed by Parliament . Although the Proclamation of 1858 announcing the assumption of the government of India by the Crown referred to Lord Canning as \" first Viceroy and Governor-General \" , none of the Warrants appointing his successors referred to them as ' Viceroys ' , and the title , which was frequently used in Warrants dealing with precedence and in public notifications , was basically one of ceremony used in connection with the state and social functions of the Sovereign 's representative . The Governor-General continued to be the sole representative of the Crown , and the Government of India continued to be vested in the Governor-General-in-Council.[1 ]\n",
      "[start] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n"
     ]
    }
   ],
   "source": [
    "print(input_sentence)\n",
    "print(translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nZ0oZPUAA7_k",
    "outputId": "3383b4c6-41f4-4227-bc89-a947991c6c6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She gave him a book.\n",
      "[start] तिने त्याला पुस्तक दिलं [end]\n",
      "Lynn runs fast .\n",
      "[start] [UNK] [UNK] [end]\n",
      "Call this number.\n",
      "[start] हे फोन कर [end]\n",
      "I 'm not Tom 's girlfriend .\n",
      "[start] मी टॉमचा [UNK] नाहीये [end]\n",
      "Tom didn't go to the hospital.\n",
      "[start] टॉमने [UNK] गेला नाही [end]\n",
      "Take this.\n",
      "[start] हे [end]\n",
      "Ken plays soccer every day .\n",
      "[start] [UNK] दररोज [UNK] [end]\n",
      "We needed to learn French.\n",
      "[start] आपल्याला फ्रेंच गरज होतं [end]\n",
      "I go to school .\n",
      "[start] मी शाळेत शाळेत [end]\n",
      "Good girls go to heaven , bad girls go everywhere .\n",
      "[start] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [end]\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    translated = decode_sequence(input_sentence)\n",
    "    print(input_sentence)\n",
    "    print(translated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ij-r8BZdROH_"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RnvhpRIdApVV"
   },
   "outputs": [],
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pvnP5RZBSNjH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fYg9qKRARrOE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
